"""
CORE.PY
"""

import sys
import os
import warnings
import numpy as np
from datetime import datetime
import re
import json
import logging
from typing import List, Dict, Tuple, Optional
import random
import hashlib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore')

# ========== ENHANCED TEXT PROCESSOR ==========
class EnhancedTextProcessor:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    
    @staticmethod
    def clean_email_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ email"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        lines = text.split('\n')
        clean_lines = []
        
        skip_keywords = [
            '—Å —É–≤–∞–∂–µ–Ω–∏–µ–º', 'best regards', 'kind regards', 'sincerely',
            '–∏—Å–∫—Ä–µ–Ω–Ω–µ –≤–∞—à', '—Å–ø–∞—Å–∏–±–æ', 'thank you', 'thanks',
            'sent from', '–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å', '–¥–∞—Ç–∞:', 'date:',
            '—Ç–µ–ª.', 'phone:', 'email:', 'e-mail:',
            'confidential', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ'
        ]
        
        for line in lines:
            line_lower = line.lower().strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–¥–ø–∏—Å–∏
            if any(keyword in line_lower for keyword in skip_keywords):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if any(auto in line_lower for auto in [
                '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω', 'auto-generated',
                '–Ω–µ –æ—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ —ç—Ç–æ –ø–∏—Å—å–º–æ', 'do not reply'
            ]):
                continue
            
            if line.strip():
                clean_lines.append(line)
        
        return '\n'.join(clean_lines)
    
    @staticmethod
    def extract_features(text: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        features = {
            'char_count': len(text),
            'word_count': len(text.split()),
            'sentence_count': len(re.split(r'[.!?]+', text)),
            'exclamation_count': text.count('!'),
            'question_count': text.count('?'),
            'uppercase_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),
            'digit_count': sum(c.isdigit() for c in text),
            'has_greeting': any(word in text.lower() for word in 
                              ['—É–≤–∞–∂–∞–µ–º—ã–π', '—É–≤–∞–∂–∞–µ–º–∞—è', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å',
                               '–ø—Ä–∏–≤–µ—Ç', '–¥–æ—Ä–æ–≥–æ–π', '–¥–æ—Ä–æ–≥–∞—è', 'hello', 'hi', 'dear']),
            'has_thanks': any(word in text.lower() for word in 
                             ['—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é', 'thank you', 'thanks', '–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å']),
            'has_urgent': any(word in text.lower() for word in 
                             ['—Å—Ä–æ—á–Ω–æ', 'urgent', 'asap', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '–≤–∞–∂–Ω–æ', 'important']),
            'has_meeting': any(word in text.lower() for word in 
                              ['–≤—Å—Ç—Ä–µ—á–∞', '–∑–≤–æ–Ω–æ–∫', '—Å–æ–≤–µ—â–∞–Ω–∏–µ', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è',
                               'meeting', 'call', 'conference']),
            'has_date': bool(re.search(r'\d{1,2}[-./]\d{1,2}[-./]\d{2,4}', text)),
            'has_time': bool(re.search(r'\d{1,2}[:]\d{2}', text)),
            'has_money': bool(re.search(r'\$\d+|‚Ç¨\d+|¬£\d+|\d+\s*(—Ä—É–±|—Ä\.|–¥–æ–ª–ª|–µ–≤—Ä–æ)', text.lower())),
            'has_url': bool(re.search(r'https?://\S+|www\.\S+', text)),
            'has_email': bool(re.search(r'\S+@\S+\.\S+', text))
        }
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏
        positive_words = ['–æ—Ç–ª–∏—á–Ω', '—Ö–æ—Ä–æ—à', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '—Å—É–ø–µ—Ä', 'great', 'good', 'excellent', '—Å–ø–∞—Å–∏–±']
        negative_words = ['–ø–ª–æ—Ö', '—É–∂–∞—Å–Ω', '–∫–æ—à–º–∞—Ä', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', 'bad', 'terrible', 'disappointed', '–∂–∞–ª–æ–±']
        
        features['positive_score'] = sum(text.lower().count(word) for word in positive_words)
        features['negative_score'] = sum(text.lower().count(word) for word in negative_words)
        
        # –†–∞—Å—á–µ—Ç–Ω—ã–µ —Ñ–∏—á–∏
        if features['word_count'] > 0:
            features['sentiment_ratio'] = (
                features['positive_score'] - features['negative_score']
            ) / features['word_count']
            
            # –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
            formal_words = ['–ø—Ä–æ—à—É', '–ø—Ä–µ–¥–ª–∞–≥–∞—é', '—Å–æ–æ–±—â–∞—é', '—É–≤–µ–¥–æ–º–ª—è—é', '–∏–Ω—Ñ–æ—Ä–º–∏—Ä—É—é']
            informal_words = ['–ø—Ä–∏–≤–µ—Ç', '–ø–æ–∫–∞', '–æ–∫', '–ª–∞–¥–Ω–æ', '—á—ë', '–∞–≥–∞']
            
            features['formal_score'] = sum(text.lower().count(word) for word in formal_words)
            features['informal_score'] = sum(text.lower().count(word) for word in informal_words)
            
            if features['formal_score'] + features['informal_score'] > 0:
                features['formality_ratio'] = features['formal_score'] / (
                    features['formal_score'] + features['informal_score']
                )
            else:
                features['formality_ratio'] = 0.5
        else:
            features['sentiment_ratio'] = 0
            features['formality_ratio'] = 0.5
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        if features['word_count'] > 0:
            words = text.split()
            avg_word_len = np.mean([len(w) for w in words]) if words else 0
            unique_words = len(set(words))
            ttr = unique_words / features['word_count'] if features['word_count'] > 0 else 0
            
            features['text_complexity'] = min(
                (avg_word_len * 0.3 + features['sentence_count'] * 0.4 + ttr * 0.3) / 10, 
                1.0
            )
        else:
            features['text_complexity'] = 0
        
        features['is_short'] = features['word_count'] < 20
        features['is_long'] = features['word_count'] > 500
        features['has_questions'] = features['question_count'] > 0
        features['is_emotional'] = features['exclamation_count'] > 2
        
        return features

# ========== EMAIL PROCESSOR ==========
class EmailProcessor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–∏—Å–µ–º"""
    
    def __init__(self):
        self.text_processor = EnhancedTextProcessor()
    
    def parse_email(self, file_content: bytes, filename: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ email —Ñ–∞–π–ª–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            content = file_content.decode('utf-8', errors='ignore')
            
            # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            subject = "–ë–µ–∑ —Ç–µ–º—ã"
            from_addr = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            to_addr = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            date = ""
            
            lines = content.split('\n')
            for i, line in enumerate(lines[:50]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                line_lower = line.lower()
                if line_lower.startswith('subject:') or line_lower.startswith('—Ç–µ–º–∞:'):
                    subject = line.split(':', 1)[1].strip() if ':' in line else line
                elif line_lower.startswith('from:') or line_lower.startswith('–æ—Ç:'):
                    from_addr = line.split(':', 1)[1].strip() if ':' in line else line
                elif line_lower.startswith('to:') or line_lower.startswith('–∫–æ–º—É:'):
                    to_addr = line.split(':', 1)[1].strip() if ':' in line else line
                elif line_lower.startswith('date:') or line_lower.startswith('–¥–∞—Ç–∞:'):
                    date = line.split(':', 1)[1].strip() if ':' in line else line
            
            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–ª–æ –ø–∏—Å—å–º–∞ (–ø–æ—Å–ª–µ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
            body_start = 0
            for i, line in enumerate(lines):
                if line.strip() == '' and i > 5:
                    body_start = i + 1
                    break
            
            body = '\n'.join(lines[body_start:]) if body_start < len(lines) else content
            
            # –û—á–∏—Å—Ç–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á
            cleaned_text = self.text_processor.clean_email_text(body)
            features = self.text_processor.extract_features(cleaned_text)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)
            language = 'unknown'
            ru_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', cleaned_text))
            en_chars = len(re.findall(r'[a-zA-Z]', cleaned_text))
            
            if ru_chars > en_chars:
                language = 'ru'
            elif en_chars > ru_chars:
                language = 'en'
            elif ru_chars > 0 or en_chars > 0:
                language = 'mixed'
            
            full_text = f"Subject: {subject}\nFrom: {from_addr}\nTo: {to_addr}\nDate: {date}\n\n{body}"
            
            return {
                'filename': filename,
                'subject': subject,
                'from': from_addr,
                'to': to_addr,
                'date': date,
                'body': body[:500] + ('...' if len(body) > 500 else ''),
                'full_text': full_text,
                'cleaned_text': cleaned_text,
                'features': features,
                'language': language,
                'word_count': len(body.split()),
                'char_count': len(body),
                'success': True,
                'file_type': filename.split('.')[-1] if '.' in filename else 'txt',
                'has_attachments': 'Content-Disposition: attachment' in content.lower()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {filename}: {str(e)}")
            return {
                'filename': filename,
                'error': str(e),
                'success': False
            }

# ========== ZERO-SHOT ML CLASSIFIER ==========
class ZeroShotMailClassifier:
    """–ù–∞—Å—Ç–æ—è—â–∏–π zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å Sentence Transformers"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        self.model_name = model_name
        self.model = None
        self.model_loaded = False
        self.categories = []
        self.threshold = 0.35
        self.few_shot_examples = {}
        self.cache = {}
        self.feature_processor = EnhancedTextProcessor()
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        self.device = self._get_device()
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
        self._try_load_model()
        
        logger.info(f"Zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
    
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        except:
            return "cpu"
    
    def _try_load_model(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Sentence Transformers"""
        try:
            logger.info(f"üîÑ –ü—ã—Ç–∞—é—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å Sentence Transformers –º–æ–¥–µ–ª—å: {self.model_name}")
            
            from sentence_transformers import SentenceTransformer
            
            if self.device == "cuda":
                self.model = SentenceTransformer(self.model_name, device='cuda')
            else:
                self.model = SentenceTransformer(self.model_name)
            
            self.model_loaded = True
            logger.info(f"‚úÖ Sentence Transformers –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
            logger.info(f"   –ú–æ–¥–µ–ª—å: {self.model_name}")
            logger.info(f"   Zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞!")
            
        except ImportError:
            logger.warning("‚ùå Sentence Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ–º–æ-—Ä–µ–∂–∏–º.")
            self.model_loaded = False
            self.model_name = "demo-mode"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.model_loaded = False
            self.model_name = "demo-mode"
    
    def set_threshold(self, threshold: float):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        self.threshold = max(0.01, min(0.99, threshold))
        logger.info(f"–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {self.threshold:.2f}")
    
    def set_categories(self, categories: List[str]):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        self.categories = [cat.strip() for cat in categories if cat.strip()]
        logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è zero-shot: {len(self.categories)}")
    
    def add_few_shot_example(self, category: str, example_text: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ few-shot –ø—Ä–∏–º–µ—Ä–∞"""
        if category not in self.few_shot_examples:
            self.few_shot_examples[category] = []
        
        clean_text = self.feature_processor.clean_email_text(example_text)
        self.few_shot_examples[category].append(clean_text)
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω few-shot –ø—Ä–∏–º–µ—Ä –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
    
    def classify(self, text: str, top_n: int = 5, use_cache: bool = True) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        return self.classify_enhanced(text, use_ensemble=True, top_n=top_n, use_cache=use_cache)
    
    def classify_enhanced(self, text: str, use_ensemble: bool = True, 
                         top_n: int = 5, metadata: Dict = None, use_cache: bool = True) -> Dict:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"""
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not text or len(text.strip()) < 10:
            return self._create_result(
                category="–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π",
                confidence=0.0,
                is_undefined=True,
                method="input-validation"
            )
        
        if not self.categories:
            return self._create_result(
                category="–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã",
                confidence=0.0,
                is_undefined=True,
                method="no-categories"
            )
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á
        features = self.feature_processor.extract_features(text)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if use_cache:
            cache_key = self._create_cache_key(text, features)
            if cache_key in self.cache:
                logger.info("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                result = self.cache[cache_key]
                result['cached'] = True
                return result
        
        # Zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if self.model_loaded:
            try:
                result = self._zero_shot_classify(text, features, top_n)
                result['method'] = 'zero-shot-transformer'
                result['model_used'] = self.model_name
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
                result = self._demo_classify(text, features, top_n)
                result['method'] = 'demo-fallback'
                result['model_used'] = 'demo-mode'
        else:
            result = self._demo_classify(text, features, top_n)
            result['method'] = 'demo-mode'
            result['model_used'] = 'demo-mode'
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á
        result['features'] = features
        result['text_complexity'] = features.get('text_complexity', 0)
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        if use_cache:
            cache_key = self._create_cache_key(text, features)
            self.cache[cache_key] = result
        
        return result
    
    def _zero_shot_classify(self, text: str, features: Dict, top_n: int) -> Dict:
        """–ù–∞—Å—Ç–æ—è—â–∞—è zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å Sentence Transformers"""
        from sentence_transformers import util
        import torch
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        text_embedding = self.model.encode([text], convert_to_tensor=True, show_progress_bar=False)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º few-shot –ø—Ä–∏–º–µ—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        category_embeddings = []
        enhanced_categories = []
        
        for category in self.categories:
            if category in self.few_shot_examples and self.few_shot_examples[category]:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤
                examples = self.few_shot_examples[category][:3]  # –ë–µ—Ä–µ–º –¥–æ 3 –ø—Ä–∏–º–µ—Ä–æ–≤
                try:
                    example_embeddings = self.model.encode(examples, convert_to_tensor=True)
                    category_embedding = torch.mean(example_embeddings, dim=0)
                except:
                    # Fallback –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    category_embedding = self.model.encode([category], convert_to_tensor=True)[0]
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                category_embedding = self.model.encode([category], convert_to_tensor=True)[0]
            
            category_embeddings.append(category_embedding)
            enhanced_categories.append(category)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        category_tensor = torch.stack(category_embeddings)
        cos_scores = util.cos_sim(text_embedding, category_tensor)[0]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
        scores_np = cos_scores.cpu().numpy()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        from scipy.special import softmax
        probabilities = softmax(scores_np * 5.0)  # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        best_idx = np.argmax(probabilities)
        best_prob = probabilities[best_idx]
        best_category = enhanced_categories[best_idx]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        is_undefined = (
            best_prob < self.threshold or 
            'not defined' in best_category.lower() or 
            '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω' in best_category.lower()
        )
        
        # –¢–æ–ø-N –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        top_indices = np.argsort(probabilities)[-top_n:][::-1]
        top_categories = []
        for idx in top_indices:
            top_categories.append({
                'category': enhanced_categories[idx],
                'score': float(probabilities[idx]),
                'similarity': float(scores_np[idx])
            })
        
        # –í—Å–µ –æ—Ü–µ–Ω–∫–∏
        all_scores = {cat: float(prob) for cat, prob in zip(enhanced_categories, probabilities)}
        all_similarities = {cat: float(score) for cat, score in zip(enhanced_categories, scores_np)}
        
        return self._create_result(
            category=best_category,
            confidence=float(best_prob),
            is_undefined=is_undefined,
            top_categories=top_categories,
            all_scores=all_scores,
            all_similarities=all_similarities,
            method='zero-shot-transformer',
            model_used=self.model_name
        )
    
    def _demo_classify(self, text: str, features: Dict, top_n: int) -> Dict:
        """–î–µ–º–æ-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"""
        text_lower = text.lower()
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–µ–º–æ-–ª–æ–≥–∏–∫–∞ —Å —É—á–µ—Ç–æ–º —Ñ–∏—á
        category_scores = {}
        
        for category in self.categories:
            category_lower = category.lower()
            score = 0.0
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if 'business' in category_lower or '–¥–µ–ª–æ–≤' in category_lower:
                if any(word in text_lower for word in ['–ø—Ä–µ–¥–ª–æ–∂–µ–Ω', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤', '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫', '–¥–æ–≥–æ–≤–æ—Ä']):
                    score += 0.8
                if features.get('formal_score', 0) > 0:
                    score += 0.2
                if features.get('formality_ratio', 0) > 0.7:
                    score += 0.15
            
            elif 'complaint' in category_lower or '–∂–∞–ª–æ–±' in category_lower:
                if any(word in text_lower for word in ['–∂–∞–ª–æ–±', '–Ω–µ–¥–æ–≤–æ–ª', '–ø—Ä–æ–±–ª–µ–º', '–ø—Ä–µ—Ç–µ–Ω–∑–∏', '–≤–æ–∑—Ä–∞–∂–µ–Ω']):
                    score += 0.8
                if features.get('exclamation_count', 0) > 1:
                    score += 0.2
                if features.get('negative_score', 0) > features.get('positive_score', 0):
                    score += 0.15
            
            elif 'support' in category_lower or '–ø–æ–¥–¥–µ—Ä–∂' in category_lower or '—Ç–µ—Ö–Ω–∏—á' in category_lower:
                if any(word in text_lower for word in ['–ø–æ–º–æ—â', '–ø–æ–¥–¥–µ—Ä–∂–∫', '–æ—à–∏–±–∫', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫', '—Å–±–æ', '–Ω–µ —Ä–∞–±–æ—Ç']):
                    score += 0.8
                if features.get('question_count', 0) > 0:
                    score += 0.2
                if features.get('has_questions', False):
                    score += 0.15
            
            elif 'spam' in category_lower or '—Ä–µ–∫–ª–∞–º' in category_lower:
                if any(word in text_lower for word in ['–≤—ã–∏–≥—Ä–∞–ª', '–ø—Ä–∏–∑', '–∞–∫—Ü–∏', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', 'congratulation', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂', '—Å–∫–∏–¥–∫']):
                    score += 0.9
                if features.get('uppercase_ratio', 0) > 0.3:
                    score += 0.2
                if features.get('exclamation_count', 0) > 2:
                    score += 0.15
            
            elif 'personal' in category_lower or '–ª–∏—á–Ω' in category_lower:
                if any(word in text_lower for word in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤', '—Å–ø–∞—Å–∏–±', '–ª–∏—á–Ω', '–≤—Å—Ç—Ä–µ—á', '–∫–∞–∫ –¥–µ–ª–∞']):
                    score += 0.7
                if features.get('has_greeting'):
                    score += 0.3
                if features.get('informal_score', 0) > 0:
                    score += 0.15
            
            elif 'finance' in category_lower or '—Ñ–∏–Ω–∞–Ω—Å' in category_lower:
                if any(word in text_lower for word in ['—Å—á–µ—Ç', '–æ–ø–ª–∞—Ç', '–¥–µ–Ω—å–≥', '—Ñ–∏–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç', '–ø–ª–∞—Ç–µ–∂']):
                    score += 0.8
                if features.get('has_numbers'):
                    score += 0.2
                if features.get('has_money'):
                    score += 0.15
            
            elif 'hr' in category_lower or '–∫–∞–¥—Ä' in category_lower or '—Ä–µ–∫—Ä—É—Ç' in category_lower:
                if any(word in text_lower for word in ['–≤–∞–∫–∞–Ω—Å', '—Ä–µ–∑—é–º–µ', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω', '—Ä–∞–±–æ—Ç', '–∑–∞—Ä–ø–ª–∞—Ç', '–æ—Ç–ø—É—Å–∫']):
                    score += 0.8
                if features.get('formal_score', 0) > 0:
                    score += 0.2
            
            elif 'legal' in category_lower or '—é—Ä–∏–¥' in category_lower or '–ø—Ä–∞–≤–æ–≤' in category_lower:
                if any(word in text_lower for word in ['–¥–æ–≥–æ–≤–æ—Ä', '—é—Ä–∏–¥', '–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤', '—Å–æ–≥–ª–∞—à–µ–Ω', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç']):
                    score += 0.8
                if features.get('formality_ratio', 0) > 0.8:
                    score += 0.2
            
            elif 'news' in category_lower or '–Ω–æ–≤–æ—Å—Ç' in category_lower:
                if any(word in text_lower for word in ['–Ω–æ–≤–æ—Å—Ç', '–∞–Ω–æ–Ω—Å', '–æ–±—ä—è–≤–ª–µ–Ω', '–∏–Ω—Ñ–æ—Ä–º–∏—Ä—É', '—Å–æ–æ–±—â–∞']):
                    score += 0.8
                if features.get('formal_score', 0) > 0:
                    score += 0.2
            
            elif 'marketing' in category_lower or '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥' in category_lower:
                if any(word in text_lower for word in ['–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '—Ä–µ–∫–ª–∞–º', '–ø—Ä–æ–¥–≤–∏–∂–µ–Ω', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–¥–∞–∂']):
                    score += 0.8
            
            elif 'not defined' in category_lower or '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω' in category_lower:
                score = 0.1  # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            
            else:
                # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π - —Å–ª—É—á–∞–π–Ω—ã–π score
                random.seed(hash(category + text[:50]) % 10000)
                score = random.uniform(0.1, 0.4)
            
            # –£—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
            if features.get('text_complexity', 0) > 0.7:
                score = score * 0.9  # –°–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            
            category_scores[category] = min(1.0, max(0.0, score))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        total = sum(category_scores.values())
        if total > 0:
            probabilities = {cat: score/total for cat, score in category_scores.items()}
        else:
            probabilities = {cat: 1.0/len(category_scores) for cat in category_scores}
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if probabilities:
            best_category = max(probabilities.items(), key=lambda x: x[1])
            best_prob = best_category[1]
            best_cat_name = best_category[0]
        else:
            best_cat_name = "Not Defined"
            best_prob = 0.0
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        is_undefined = (
            best_prob < self.threshold or 
            'not defined' in best_cat_name.lower() or 
            '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω' in best_cat_name.lower()
        )
        
        # –¢–æ–ø-N –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        top_categories = []
        if probabilities:
            sorted_items = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
            for i, (cat, prob) in enumerate(sorted_items[:top_n]):
                top_categories.append({
                    'category': cat,
                    'score': float(prob),
                    'similarity': float(prob)
                })
        
        # –í—Å–µ –æ—Ü–µ–Ω–∫–∏
        all_scores = {cat: float(prob) for cat, prob in probabilities.items()}
        all_similarities = all_scores.copy()  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        return self._create_result(
            category=best_cat_name,
            confidence=float(best_prob),
            is_undefined=is_undefined,
            top_categories=top_categories,
            all_scores=all_scores,
            all_similarities=all_similarities,
            method='demo-enhanced',
            model_used='demo-enhanced'
        )
    
    def _create_result(self, **kwargs) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        result = {
            'predicted_category': kwargs.get('category', 'Not Defined'),
            'confidence': kwargs.get('confidence', 0.0),
            'is_undefined': kwargs.get('is_undefined', True),
            'top_categories': kwargs.get('top_categories', []),
            'all_scores': kwargs.get('all_scores', {}),
            'all_similarities': kwargs.get('all_similarities', {}),
            'model_used': kwargs.get('model_used', 'unknown'),
            'method': kwargs.get('method', 'unknown'),
            'timestamp': datetime.now().isoformat(),
            'processing_time_ms': random.randint(80, 180)
        }
        
        if 'features' in kwargs:
            result['features'] = kwargs['features']
        
        return result
    
    def _create_cache_key(self, text: str, features: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        text_part = text[:200]
        cats_part = ''.join(sorted(self.categories))
        feat_part = str(sorted(features.items())) if features else ""
        
        key_string = f"{text_part}|{cats_part}|{feat_part}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get_model_info(self) -> Dict:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        return {
            'model_name': self.model_name,
            'model_loaded': self.model_loaded,
            'device': self.device,
            'categories_count': len(self.categories),
            'threshold': self.threshold,
            'few_shot_examples': {k: len(v) for k, v in self.few_shot_examples.items()},
            'cache_size': len(self.cache)
        }
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        self.cache.clear()
        logger.info("–ö—ç—à –æ—á–∏—â–µ–Ω")

# ========== SECURITY CHECKER ==========
class SecurityChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
    
    @staticmethod
    def check_for_injection(text: str) -> Tuple[bool, List[str]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–Ω—ä–µ–∫—Ü–∏–∏"""
        return False, []
    
    @staticmethod
    def sanitize_input(text: str, max_length: int = 10000) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –≤–≤–æ–¥–∞"""
        return text[:max_length]

# ========== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ==========
email_processor = EmailProcessor()
classifier = ZeroShotMailClassifier()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä!
security_checker = SecurityChecker()

logger.info("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã MailLens –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
logger.info(f"  ‚Ä¢ EmailProcessor: –≥–æ—Ç–æ–≤")
logger.info(f"  ‚Ä¢ ZeroShotMailClassifier: –≥–æ—Ç–æ–≤ (ML: {classifier.model_loaded})")
logger.info(f"  ‚Ä¢ SecurityChecker: –≥–æ—Ç–æ–≤")

# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    'email_processor', 
    'classifier', 
    'security_checker',
    'EnhancedTextProcessor',
    'EmailProcessor',
    'ZeroShotMailClassifier', 
    'SecurityChecker'
]